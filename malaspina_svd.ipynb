{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7418a3d8",
   "metadata": {},
   "source": [
    "# Malaspina stochastic matrix factorization\n",
    "\n",
    "This method is based off the paper found here: https://proceedings.neurips.cc/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf\n",
    "\n",
    "First, import the data cube and draw 1000 random velocity fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as ncf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "fn = './MalaspinaGlacierCube_32607.nc'\n",
    "data = ncf.Dataset(fn)\n",
    "\n",
    "# This makes fields with a higher number of non-masked entries more probable\n",
    "temperature=10.0\n",
    "\n",
    "# Precomputed non-masked fraction\n",
    "np.random.seed(0)\n",
    "valid_fraction = np.array(pickle.load(open('valid_fraction.p','rb')))\n",
    "\n",
    "draw_p = valid_fraction**temperature\n",
    "draw_p /= draw_p.sum()\n",
    "\n",
    "# Draw a set of indices and sort them by date\n",
    "inds = np.random.choice(range(len(valid_fraction)),size=min(1000,len(valid_fraction)),p=draw_p,replace=False)\n",
    "inds = inds[np.argsort(data.variables['date_center'][inds])]\n",
    "\n",
    "# Working with magnitudes\n",
    "U = data.variables['v']#[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058ead6",
   "metadata": {},
   "source": [
    "This next line actually loads the data into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_inds = U[inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf059dd8",
   "metadata": {},
   "source": [
    "Compute time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = data.variables['date_center'][inds]\n",
    "dt = dates[1:] - dates[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedbe25",
   "metadata": {},
   "source": [
    "Import a glacier outline for use as a mask.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t,n_row,n_col = U_trunc.shape\n",
    "\n",
    "boundary_points = pickle.load(open('boundary.p','rb'))\n",
    "xs = data.variables['x'][50:300]\n",
    "ys = data.variables['y'][:300]\n",
    "\n",
    "Xs,Ys = np.meshgrid(xs,ys)\n",
    "X = np.vstack((Xs.ravel(),Ys.ravel())).T\n",
    "\n",
    "import pyproj\n",
    "import matplotlib.path as path\n",
    "tform = pyproj.Transformer.from_crs(crs_from=3338, crs_to=32607, always_xy=True)\n",
    "fx, fy = tform.transform(boundary_points[:,0], boundary_points[:,1])\n",
    "\n",
    "p = path.Path(np.vstack((fx,fy)).T)\n",
    "glacier_mask = p.contains_points(X)\n",
    "\n",
    "plt.scatter(*X[glacier_mask].T)\n",
    "\n",
    "glacier_mask = np.hstack([glacier_mask]*n_t).reshape(n_t,n_row,n_col)\n",
    "mask_f = glacier_mask.reshape(U_trunc.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba2a53",
   "metadata": {},
   "source": [
    "Compute the mean of off-ice but still \"valid\" pixels for each image and subtract this from the observed velocity's valid pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate to a slightly smaller area to avoid weirdness in the ocean\n",
    "U_trunc = U_inds.filled(-1).astype(float)[:,:300,50:300]\n",
    "\n",
    "off_ice_means = np.zeros(U_trunc.shape[0])\n",
    "for i in range(U_trunc.shape[0]):\n",
    "    \n",
    "    off_ice_means[i] = np.median(U_trunc[i][(U_trunc[i]!=-1) & (glacier_mask[i]==0)])\n",
    "    U_trunc[i][U_trunc[i]!=-1] -= off_ice_means[i]\n",
    "    \n",
    "Uf = (U_trunc).reshape(U_trunc.shape[0],-1).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc640848",
   "metadata": {},
   "source": [
    "Solve a probabilistic matrix factorization problem, with a few types of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many non-orthogonal modes to compute\n",
    "l = 30\n",
    "\n",
    "# Regularization strength on column-space (spreads out mode coefficients)\n",
    "lamda_u = 10.0\n",
    "\n",
    "# Regularization strength on row-space (makes it more likely to use more modes to describe a velocity field)\n",
    "lamda_v = 1.0\n",
    "\n",
    "# Penalizes mode gradients\n",
    "lamda_x = 1000.0\n",
    "\n",
    "# Penalizes variation in mode strength through time\n",
    "lamda_t = 10000.0\n",
    "\n",
    "# avoid divide by zero in time step (which is used in time-gradient regularization)\n",
    "dt_0 = torch.from_numpy(dt + 1.0)\n",
    "\n",
    "# Do some reshaping and rescaling\n",
    "R = torch.from_numpy(Uf)\n",
    "Rhat = R.ravel()\n",
    "I_0 = Rhat!=-1.\n",
    "I_1 = torch.from_numpy(mask_f.ravel())\n",
    "I = I_0 & I_1\n",
    "Rbar = Rhat[I]\n",
    "\n",
    "Rmean = Rbar.mean()\n",
    "Rstd = Rbar.std()\n",
    "\n",
    "Rbar = (Rbar - Rmean)/Rstd\n",
    "\n",
    "# Define low-rank matrix factors \n",
    "U_ = torch.randn(R.shape[0],l,requires_grad=True)\n",
    "V_ = torch.randn(l,R.shape[1],requires_grad=True)\n",
    "\n",
    "# Initialize\n",
    "U_.data[:] *= 1e-2\n",
    "V_.data[:] *= 1e-2\n",
    "\n",
    "# Define an optimizer for gradient descent\n",
    "optimizer = torch.optim.Adam([U_,V_],1e-1)\n",
    "\n",
    "# Do 500 iterations of gradient descent\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the predicted matrix\n",
    "    G = U_ @ V_\n",
    "   \n",
    "    # Compute the gradients of the right factor (the mode coefficients through time)\n",
    "    dudt = (V_[:,1:] - V_[:,:-1])/dt_0\n",
    "    \n",
    "    # Reshape the left factor (columns hold the modes) into a grid and takes its gradients.\n",
    "    U_grid = U_.T.reshape(l,n_row,n_col)    \n",
    "    dudrow = U_grid[:,1:] - U_grid[:,:-1]\n",
    "    dudcol = U_grid[:,:,1:] - U_grid[:,:,:-1]\n",
    "    \n",
    "    # Flatten and mask the predictions\n",
    "    Gbar = G.ravel()[I]\n",
    "\n",
    "    # Compute a variety of negative log likelihoods\n",
    "    \n",
    "    # Data misfit\n",
    "    E_misfit = ((Rbar - Gbar)**2).mean() \n",
    "    #E_misfit = (torch.abs(Rbar - Gbar)).mean() \n",
    "    \n",
    "    # norm regularization\n",
    "    E_reg = lamda_u*(U_**2).sum()/len(Rbar) + lamda_v*(V_**2).sum()/len(Rbar)\n",
    "    \n",
    "    # Spatial gradient regularization\n",
    "    E_space = lamda_x/len(Rbar)*((dudrow**2).sum() + (dudcol**2).sum())  \n",
    "    \n",
    "    # Time gradient regularization\n",
    "    E_time = lamda_t/len(Rbar)*(dudt**2).sum()\n",
    "    \n",
    "    # Sum to form total cost\n",
    "    E = E_misfit + E_reg + E_space + E_time\n",
    "    \n",
    "    # Backpropagate gradients\n",
    "    E.backward()\n",
    "    \n",
    "    # Update factors\n",
    "    optimizer.step()\n",
    "    print(i,E_misfit.item(),E_reg.item(),E_space.item(),E_time.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbd041",
   "metadata": {},
   "source": [
    "The factors are non-orthogonal, which is a pain to look at.  However, now that the reconstruction doesn't have any missing data, we can just apply low-rank SVD to grab the first 10 or so modes very efficiently (which are now orthonormal).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed5c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,v = torch.svd_lowrank(U_.detach()@V_.detach(),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2cb03",
   "metadata": {},
   "source": [
    "Plot the columns of $u$.  Mode importance is given by $s$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(u[:,16].reshape(U_trunc.shape[1],U_trunc.shape[2]))\n",
    "#plt.gcf().set_size_inches(12,12)\n",
    "for i in range(10):\n",
    "    plt.imshow(u[:,i].detach().reshape(U_trunc.shape[1],U_trunc.shape[2]),extent=(xs.min(),xs.max(),ys.min(),ys.max()),cmap=plt.cm.seismic,vmin=-0.03,vmax=0.03)\n",
    "    plt.scatter(fx,fy)\n",
    "    plt.xlim(xs.min(),xs.max())\n",
    "    plt.ylim(ys.min(),ys.max())\n",
    "    plt.title(f\"Mode {i}, Variance Fraction {s.numpy()[i]/s.numpy()[0]*100:.01f}%\")\n",
    "    plt.gcf().set_size_inches(12,12)\n",
    "    plt.savefig(f'modes_biascorrected/mode_{i:02d}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f46ea",
   "metadata": {},
   "source": [
    "Reconstruct the velocity fields from the truncated SVD (this gets rid of scanline modes and other undesirables).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41214d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_recon = ((u * s) @ v.T).T.reshape(n_t,n_row,n_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d3fec5",
   "metadata": {},
   "source": [
    "Plot the reconstructed velocity fields along with the original fields.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(n_t):\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=2)\n",
    "    axs[0].imshow((torch.maximum(U_recon[i]*Rstd + Rmean,torch.ones(n_row,n_col))),vmin=0,vmax=2000,extent=(xs.min(),xs.max(),ys.min(),ys.max()))\n",
    "    axs[0].plot(fx,fy,'r-')\n",
    "    axs[0].set_xlim(xs.min(),xs.max())\n",
    "    axs[0].set_ylim(ys.min(),ys.max())\n",
    "    #plt.colorbar()\n",
    "    axs[1].imshow((U_trunc[i]),vmin=0,vmax=2000,extent=(xs.min(),xs.max(),ys.min(),ys.max()))\n",
    "    axs[1].plot(fx,fy,'r-')\n",
    "    axs[1].set_xlim(xs.min(),xs.max())\n",
    "    axs[1].set_ylim(ys.min(),ys.max())\n",
    "    fig.set_size_inches(12,8)\n",
    "    plt.subplots_adjust(wspace=0.0)\n",
    "    axs[0].set_title(f't={1985+(dates[i]-dates[0])/365.:.02f}')\n",
    "    fig.savefig(f'out_images_biascorrected/vel_{i:03d}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a0379",
   "metadata": {},
   "source": [
    "Plot a time series of the top 7 modes (with arbitrary offset to see the patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(1985+(dates-dates[0])/365,v[:,i]+i*0.2,color=plt.cm.viridis(i/7.))\n",
    "ax = plt.gca()\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xlabel('Years since 1985')\n",
    "plt.savefig('time_series.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28b079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
